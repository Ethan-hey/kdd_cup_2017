{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import normali"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = [-3, -2, -1, 1, 2, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MinMaxScaler(copy=True, feature_range=[-3, -2, -1, 1, 2, 3])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.MinMaxScaler(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\sklearn\\utils\\validation.py:395: DeprecationWarning: Passing 1d arrays as data is deprecated in 0.17 and will raise ValueError in 0.19. Reshape your data either using X.reshape(-1, 1) if your data has a single feature or X.reshape(1, -1) if it contains a single sample.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([-0.56694671, -0.37796447, -0.18898224,  0.18898224,  0.37796447,\n",
       "        0.56694671])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.normalize(a)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.38873015, -0.9258201 , -0.46291005,  0.46291005,  0.9258201 ,\n",
       "        1.38873015])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocessing.scale(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAPE(preds, outputs):\n",
    "    preds = np.array(preds)\n",
    "    outputs = np.array(outputs)\n",
    "    return np.average(np.abs(outputs - preds) / outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def date_converter(volume, weather):\n",
    "    # Initialize lists to store splitted information\n",
    "    date_list = []\n",
    "    month_list = []\n",
    "    day_list = []\n",
    "    hour_list = []\n",
    "    minute_list = []\n",
    "    rounded_min_list = []\n",
    "    rounded_hour_list = []\n",
    "\n",
    "    # Splits the 'time' information into month, day, hour and minute\n",
    "    def date_spliter(date):\n",
    "        counter = 0\n",
    "        parts = date.split(\" \")\n",
    "        day_part = parts[0]\n",
    "        clock_part = parts[1]\n",
    "\n",
    "        day_parts = day_part.split(\"-\")\n",
    "        month = int(day_parts[1]) # Extract month from time\n",
    "        day = int(day_parts[2]) # Extract day from time\n",
    "\n",
    "        clock_parts = clock_part.split(\":\")\n",
    "        hour = int(clock_parts[0]) # Extract hour from time\n",
    "        minute = int(clock_parts[1]) # Extract minute from time\n",
    "\n",
    "        rounded_hour = str(hour // 3 * 3)\n",
    "        rounded_min = str(minute // 20 * 20)\n",
    "\n",
    "        date_list.append(day_part)\n",
    "        month_list.append(month)\n",
    "        day_list.append(day)\n",
    "        hour_list.append(hour)\n",
    "        minute_list.append(minute)\n",
    "        rounded_hour_list.append(rounded_hour)\n",
    "        rounded_min_list.append(rounded_min)\n",
    "\n",
    "    # Store info into lists\n",
    "    for date in volume['time']:\n",
    "        date_spliter(date)\n",
    "\n",
    "    # Add arrays into the 'volume' SFrame\n",
    "    volume['month'] = np.array(month_list)\n",
    "    volume['day'] = np.array(day_list)\n",
    "    volume['hour'] = np.array(hour_list)\n",
    "    volume['minute'] = np.array(minute_list)\n",
    "    volume['date'] = np.array(date_list)\n",
    "    volume['rounded_hour'] = np.array(rounded_hour_list)\n",
    "    volume['rounded_min'] = np.array(rounded_min_list)\n",
    "    \n",
    "    # Add an colume which combine 'date' and 'rounded_hour'\n",
    "    print type(volume['date'][0])\n",
    "    slash_list = np.array(['-'] * len(volume['date']))\n",
    "    volume['date_and_rounded_hour'] = volume['date'] + slash_list + volume['rounded_hour']\n",
    "    \n",
    "    slash_list = np.array(['-'] * len(weather['date']))\n",
    "    weather['date_and_rounded_hour'] = weather['date'] + slash_list + np.array([str(hour) for hour in weather['hour']])\n",
    "    \n",
    "    return (volume, weather)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def build_merged_tables(volume, weather, join_type='inner'):\n",
    "    # Merge 'volume' and 'weather' DataFrame together\n",
    "    volume_weather = pd.merge(volume, weather, on='date_and_rounded_hour', suffixes=('', '_y'))\n",
    "\n",
    "    # Construct 'window_time' list which uses date, hour, and rounded minute\n",
    "    date_list = volume_weather['date'] + np.array(['-'] * len(volume_weather))\n",
    "    hour_list = volume_weather['hour'].astype(str) + np.array(['-'] * len(volume_weather))\n",
    "    window_time_list = date_list + hour_list + volume_weather['rounded_min']\n",
    "    volume_weather['window_time'] = window_time_list\n",
    "    \n",
    "    return volume_weather"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Group 'volume_weather' here\n",
    "def group_vw(volume_weather):\n",
    "    \n",
    "    volume_weather = volume_weather.groupby(['window_time', 'tollgate_id', 'direction'])\n",
    "    vwgrouped = volume_weather.agg('mean').join(pd.DataFrame(volume_weather.size(), columns=['count']))\n",
    "\n",
    "    # Put index as column\n",
    "    vwgrouped['direction'] = vwgrouped.index.get_level_values('direction')\n",
    "    vwgrouped['tollgate_id'] = vwgrouped.index.get_level_values('tollgate_id')\n",
    "    vwgrouped['window_time'] = vwgrouped.index.get_level_values('window_time')\n",
    "    \n",
    "    return vwgrouped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def re_construct_data(volume_weather):\n",
    "\n",
    "    # Create 'weekday' column in DateFrame(0 stands for Sunday; 1 stands for Monday and 2 stands for Tuesday, etc...)\n",
    "    sept = volume_weather[volume_weather['month'] == 9]\n",
    "    weekday1 = ((sept['day'] + 3) % 7).values\n",
    "    octo = volume_weather[volume_weather['month'] == 10]\n",
    "    weekday2 = ((octo['day'] + 5) % 7).values\n",
    "    volume_weather['weekday'] = np.append(weekday1, weekday2)\n",
    "    \n",
    "    # Create 'rounded_min' column\n",
    "    volume_weather['rounded_min'] = volume_weather['minute'] // 20 * 20\n",
    "    \n",
    "    # Construc splitted numerical data column\n",
    "    def split_value(df, column, values_list):\n",
    "        for i in range(len(values_list) - 1):\n",
    "            begin = values_list[i]\n",
    "            end = values_list[i+1]\n",
    "            new_column_name = column + \"_\" + str(begin) + \"_\" + str(end)\n",
    "            df[new_column_name] = (df[column] >= begin).astype(int) & (df[column] < end).astype(int)\n",
    "        return df\n",
    "    \n",
    "    # Create 'is_column_x' column in DataFrame\n",
    "    def create_is_columns(df, column_names):\n",
    "        for column_name in column_names:\n",
    "            for i in np.sort(df[column_name].unique()):\n",
    "                new_column_name = \"is_\" + column_name + \"_\" + str(i)\n",
    "                df[new_column_name] = (df[column_name] == i).astype(int)\n",
    "        return df\n",
    "\n",
    "    is_columns = ['rounded_min', 'hour', 'tollgate_id', 'direction', 'weekday' ]\n",
    "    create_is_columns(volume_weather, is_columns)\n",
    "\n",
    "    # Construct 'is_festival' column\n",
    "    volume_weather['is_festival'] = np.array([0] * len(volume_weather))\n",
    "    sep_days = [15, 16, 17]\n",
    "    oct_days = [1, 2, 3, 4, 5, 6, 7]\n",
    "    for day in sep_days:\n",
    "        volume_weather.loc[((volume_weather['month'] == 9) & (volume_weather['day'] == day)), 'is_festival'] = 1\n",
    "    for day in oct_days:\n",
    "        volume_weather.loc[((volume_weather['month'] == 10) & (volume_weather['day'] == day)), 'is_festival'] = 1\n",
    "        \n",
    "    # Construct 'is_working_day' column\n",
    "    volume_weather['is_working_day'] = np.array([0] * len(volume_weather))\n",
    "    volume_weather.loc[((volume_weather['weekday'] < 5) & (volume_weather['weekday'] > 0)), 'is_working_day'] = 1\n",
    "    volume_weather.loc[volume_weather['is_festival'] == 1, 'is_working_day'] = 0\n",
    "    volume_weather.loc[((volume_weather['month'] == 9) & (volume_weather['day'] == 18)), 'is_working_day'] = 1\n",
    "    volume_weather.loc[((volume_weather['month'] == 10)\n",
    "                        & ((volume_weather['day'] == 8) | (volume_weather['day'] == 9))), 'is_working_day'] = 1\n",
    "\n",
    "    \n",
    "    return volume_weather"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Show subplots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# axis, fig = plt.subplots(2, 3, pltsize = ())\n",
    "# axis(1, 0) = plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-fa10c77e7554>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0max1\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m211\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0max2\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_subplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m212\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0max1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvwgrouped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'pressure'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvwgrouped\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'count'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "# fig = plt.figure()\n",
    "# ax1 = fig.add_subplot(211)\n",
    "# ax2 = fig.add_subplot(212)\n",
    "\n",
    "# ax1.plot(vwgrouped['pressure'], vwgrouped['count'], '.')\n",
    "# print np.corrcoef(vwgrouped['pressure'], vwgrouped['count'])\n",
    "\n",
    "# ax2.plot(nf_vwgroup['pressure'], nf_vwgroup['count'], '.')\n",
    "# print np.corrcoef(nf_vwgroup['pressure'], nf_vwgroup['count'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_LR_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=True):\n",
    "    total_alphas = []\n",
    "#     mapes_validation = []\n",
    "    LR_models = []\n",
    "    total_val_mapes = []\n",
    "    total_test_mapes = []\n",
    "    for i in range(20):\n",
    "        (alpha, mape_valid, reg) = fit_model_LinearReg(Xs_train_valid[i], Ys_train_valid[i])\n",
    "\n",
    "#         alphas.append(alpha)\n",
    "#         mapes_validation.append(mape_valid)\n",
    "        LR_models.append(reg)\n",
    "\n",
    "        X_test = Xs_test[i]\n",
    "        Y_test = Ys_test[i]\n",
    "        pred = reg.predict(X_test)\n",
    "        mape_test = MAPE(pred, Y_test)\n",
    "\n",
    "        total_val_mapes.append(mape_valid)\n",
    "        total_test_mapes.append(mape_test)\n",
    "        total_alphas.append(alpha)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('alpha: {0:.2f}'.format(alpha), 'mape_val: {0:.2f}'.format(mape_valid),\n",
    "                   'mape_test: {0:.2f}'.format(mape_test))\n",
    "    ave_mape_valid = np.average(total_val_mapes)\n",
    "    ave_mape_test = np.average(total_test_mapes)\n",
    "    ave_alpha = np.average(total_alphas)\n",
    "    print 'Validation mape:', '{0:.2f}'.format(ave_mape_valid)\n",
    "    print 'Test mape:', '{0:.2f}'.format(ave_mape_test)\n",
    "    print 'Alpha:', '{0:.2f}'.format(ave_alpha)\n",
    "    print '-------------------End of training---------------------------'\n",
    "    return (ave_mape_valid, ave_mape_test, LR_models, ave_alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessingrocess import normali"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
