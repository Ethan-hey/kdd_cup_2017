{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. add 'is_festival' features\n",
    "2. try to split features into 'is_8_20min', 'is_8_40min' etc. \n",
    "3. observe ratio between hour 7 and hour 8, ect. Check if the tendancy ratio vary rapidly among festival period\n",
    "4. use ratio*volume to create some new features? \\\n",
    "5. deal with missing data: no data for some day"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?np.corrcoef"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import datetime\n",
    "%matplotlib inline\n",
    "pd.set_option('display.max_columns', 500)\n",
    "import datetime\n",
    "from __future__ import division\n",
    "import seaborn as sns\n",
    "from sklearn import linear_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "my_submission = pd.read_csv(\"predictions/RF_prediction_428.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'DataFrame' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-e2ed0505b4d2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmy_submission\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmy_submission\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'volume'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'.'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: 'DataFrame' object is not callable"
     ]
    }
   ],
   "source": [
    "plt.plot(range(len(my_submission)), my_submission('volume'), '.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def MAPE(preds, outputs):\n",
    "    preds = np.array(preds)\n",
    "    outputs = np.array(outputs)\n",
    "    return np.average(np.abs(outputs - preds) / outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combi_in = pd.read_csv(\"combi_in.csv\")\n",
    "combi_out = pd.read_csv(\"combi_out.csv\")\n",
    "combi_in = combi_in[combi_in['hour'].isin([6, 7, 8, 9, 15, 16, 17, 18])][['window_time', 'window_time_formatted', 'month',\n",
    "                                                                    'day', 'hour', 'tollgate_id', 'direction', 'count',\n",
    "                                                                          'minute']]\n",
    "combi_out = combi_out[combi_out['hour'].isin([6, 7, 8, 9, 15, 16, 17, 18])][['window_time', 'window_time_formatted', 'month',\n",
    "                                                                    'day', 'hour', 'tollgate_id', 'direction', 'count', \n",
    "                                                                            'minute']]\n",
    "\n",
    "\n",
    "submission = pd.read_csv('submission_sample_volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combi_in['rounded_min'] = combi_in['minute'] // 20 * 20\n",
    "combi_out['rounded_min'] = combi_out['minute'] // 20 * 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi_in['window_time_formatted'] = combi_in['window_time_formatted'].apply(\n",
    "                    lambda t : datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')).tolist()\n",
    "combi_out['window_time_formatted'] = combi_out['window_time_formatted'].apply(\n",
    "                    lambda t : datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combi_in_1 = combi_in[combi_in['tollgate_id'] == 1]\n",
    "combi_in_2 = combi_in[combi_in['tollgate_id'] == 2]\n",
    "combi_in_3 = combi_in[combi_in['tollgate_id'] == 3]\n",
    "combi_out_1 = combi_out[combi_out['tollgate_id'] == 1]\n",
    "combi_out_3 = combi_out[combi_out['tollgate_id'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Move extreme values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_peak_values(df, hour):\n",
    "    # Data in selected hour\n",
    "    df_hour = df[df['hour'] == hour]\n",
    "    # Normal data\n",
    "    norm = np.percentile(df_hour['count'], 50)\n",
    "    # Threshold value for filtering peak values\n",
    "    threshold = 2 * norm\n",
    "    # Peak values\n",
    "    peak_values = df_hour[df_hour['count'] > threshold]['count']\n",
    "    # Terminate in advance\n",
    "    if len(peak_values) == 0:\n",
    "        return df\n",
    "    # Ratio used to adjust data\n",
    "    ratio = np.percentile(peak_values, 50) / np.percentile(df_hour['count'], 50)\n",
    "#     print ratio\n",
    "\n",
    "    df.loc[(df['hour'] == hour) & (df['count'] > threshold), 'count'] = peak_values / ratio\n",
    "    return df\n",
    "\n",
    "def move_valley_values(df, hour):\n",
    "    # Data in selected hour\n",
    "    df_hour = df[df['hour'] == hour]\n",
    "    # Normal data\n",
    "    norm = np.percentile(df_hour['count'], 50)\n",
    "    # Threshold value for filtering valley values\n",
    "    threshold = norm / 2\n",
    "    # Valley values\n",
    "    valley_values = df_hour[df_hour['count'] < threshold]['count']\n",
    "    # Terminate in advance\n",
    "    if len(valley_values) == 0:\n",
    "        return df\n",
    "    # Ratio used to adjust data\n",
    "    ratio = np.percentile(valley_values, 50) / np.percentile(df_hour['count'], 50)\n",
    "#     print ratio\n",
    "\n",
    "    df.loc[(df['hour'] == hour) & (df['count'] < threshold), 'count'] = valley_values / ratio\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def move_extreme_values(df):\n",
    "    hours = [8, 9, 17, 18]\n",
    "    for hour in hours:\n",
    "        df = move_peak_values(df, hour)\n",
    "        df = move_valley_values(df, hour)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.py:477: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  self.obj[item] = s\n"
     ]
    }
   ],
   "source": [
    "combi_in_1 = move_extreme_values(combi_in_1)\n",
    "combi_in_2 = move_extreme_values(combi_in_2)\n",
    "combi_in_3 = move_extreme_values(combi_in_3)\n",
    "combi_out_1 = move_extreme_values(combi_out_1)\n",
    "combi_out_3 = move_extreme_values(combi_out_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "datas = [combi_in_1, combi_in_2, combi_in_3, combi_out_1, combi_out_3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 6 shifted time "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Create shifted time windows\n",
    "# eg: for window_time: '2016-9-18-8-20': \n",
    "#                      'ft_1': '2016-9-18-6-0'; 'ft_2': '2016-9-18-6-20'; 'ft_3': '2016-9-18-6-40'; \n",
    "#                      'ft_4': '2016-9-18-7-0'; 'ft_5': '2016-9-18-7-20'; 'ft_6': '2016-9-18-7-40'. \n",
    "# Note that '2016-9-18-8-0', '2016-9-18-8-20', '2016-9-18-8-40', '2016-9-18-9-0', '2016-9-18-9-20', '2016-9-18-9-40' share the\n",
    "# same 'ft_1' to 'ft_6' \n",
    "def create_shifted_time(datas):\n",
    "    for i in range(5):\n",
    "        data = datas[i].copy()\n",
    "\n",
    "        data['fake_hour'] = np.array([0] * len(data)).astype(str)\n",
    "        data.loc[(data['hour'] == 8) | (data['hour'] ==9), 'fake_hour'] = str(6)\n",
    "        data.loc[(data['hour'] == 17) | (data['hour'] == 18), 'fake_hour'] = str(15)\n",
    "        data['fake_hour+1'] = (data['fake_hour'].astype(int) + 1).astype(str)\n",
    "\n",
    "        data['year'] = data['window_time_formatted'].apply(lambda t : t.year).astype(str)\n",
    "        data['month'] = data['window_time_formatted'].apply(lambda t : t.month).astype(str)\n",
    "        data['day'] = data['window_time_formatted'].apply(lambda t : t.day).astype(str)\n",
    "\n",
    "        features = ['ft_1', 'ft_2', 'ft_3', 'ft_4', 'ft_5', 'ft_6']\n",
    "        slash = np.array(['-'] * len(data))\n",
    "        data['ft_1'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour'] \\\n",
    "                        + slash + np.array([0] * len(data)).astype(str)\n",
    "        data['ft_2'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour'] \\\n",
    "                        + slash + np.array([20] * len(data)).astype(str)\n",
    "        data['ft_3'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour'] \\\n",
    "                        + slash + np.array([40] * len(data)).astype(str)\n",
    "        data['ft_4'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour+1'] \\\n",
    "                        + slash + np.array([0] * len(data)).astype(str)\n",
    "        data['ft_5'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour+1'] \\\n",
    "                        + slash + np.array([20] * len(data)).astype(str)\n",
    "        data['ft_6'] = data['year'] + slash + data['month'] + slash + data['day'] + slash + data['fake_hour+1'] \\\n",
    "                        + slash + np.array([40] * len(data)).astype(str)\n",
    "\n",
    "        for feature in features:\n",
    "            data[feature] = data[feature].apply(lambda t : datetime.datetime.strptime(t, '%Y-%m-%d-%H-%M')).tolist()\n",
    "        datas[i] = data\n",
    "    return datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "datas = create_shifted_time(datas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add weather features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi_in_1 = pd.read_csv(\"combi_in_1.csv\")\n",
    "combi_in_2 = pd.read_csv(\"combi_in_2.csv\")\n",
    "combi_in_3 = pd.read_csv(\"combi_in_3.csv\")\n",
    "combi_out_1 = pd.read_csv(\"combi_out_1.csv\")\n",
    "combi_out_3 = pd.read_csv(\"combi_out_3.csv\")\n",
    "\n",
    "\n",
    "weather_features = ['window_time_formatted', \n",
    "            'wind_direction_0.0', 'wind_direction_15.0', 'wind_direction_30.0', 'wind_direction_45.0', 'wind_direction_60.0', \n",
    "              'wind_direction_75.0', 'wind_direction_90.0', 'wind_direction_105.0', 'wind_direction_120.0', \n",
    "              'wind_direction_135.0', 'wind_direction_150.0', 'wind_direction_165.0', 'wind_direction_180.0', \n",
    "              'wind_direction_195.0', 'wind_direction_210.0', 'wind_direction_225.0', 'wind_direction_240.0', \n",
    "              'wind_direction_255.0', 'wind_direction_270.0', 'wind_direction_285.0', 'wind_direction_300.0', \n",
    "              'wind_direction_315.0', 'wind_direction_330.0', 'wind_direction_345.0', 'wind_direction_360.0', \n",
    "              'wind_speed_0.0', 'wind_speed_1.0', 'wind_speed_2.0', 'wind_speed_3.0', 'wind_speed_4.0', 'wind_speed_5.0', \n",
    "              'wind_speed_6.0', 'wind_speed_7.0', 'temperature_14.0', 'temperature_16.0', 'temperature_18.0', \n",
    "              'temperature_20.0', 'temperature_22.0', 'temperature_24.0', 'temperature_26.0', 'temperature_28.0', \n",
    "              'temperature_30.0', 'temperature_32.0', 'rel_humidity_40.0', 'rel_humidity_50.0', 'rel_humidity_60.0', \n",
    "              'rel_humidity_70.0', 'rel_humidity_80.0', 'rel_humidity_90.0', 'precipitation_0.0', 'precipitation_1.0', \n",
    "              'precipitation_2.0', 'precipitation_3.0', 'precipitation_4.0', 'precipitation_5.0']\n",
    "\n",
    "combis = [combi_in_1, combi_in_2, combi_in_3, combi_out_1, combi_out_3]\n",
    "for i in range(len(combis)):\n",
    "    combi = combis[i]\n",
    "    combi['window_time_formatted'] = combi['window_time_formatted'].apply(lambda t : \n",
    "                                                                         datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S'))\n",
    "    combis[i] = combi\n",
    "\n",
    "for i in range(5):\n",
    "    data = datas[i]\n",
    "    combi = combis[i][weather_features]\n",
    "    merged = pd.merge(left=data, right=combi, on='window_time_formatted', how='inner')\n",
    "    datas[i] = merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### add 'ft_x_count' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Add 'count' feature for each of the 'ft_x' column\n",
    "def create_prev_count(datas):\n",
    "#     earlys = []\n",
    "#     lates = []\n",
    "    mers = []\n",
    "    mer_lists = []\n",
    "    ons = ['ft_1', 'ft_2', 'ft_3', 'ft_4', 'ft_5', 'ft_6']\n",
    "    for k in range(5):\n",
    "        data = datas[k]\n",
    "        early = data[data['hour'].isin([6, 7, 15, 16])]\n",
    "        late = data[data['hour'].isin([8, 9, 17, 18])]\n",
    "        mer = late.copy()\n",
    "#         earlys.append(early)\n",
    "#         lates.append(late)\n",
    "        for i in range(6):\n",
    "            mer_list = pd.merge(left=late, right=early[['window_time_formatted', 'count']],\n",
    "                       how='left', left_on=ons[i], right_on='window_time_formatted',suffixes=('', '_y'))['count_y'].tolist()\n",
    "            mer_lists.append(mer_list)\n",
    "#             print 'mer_list:', len(mer_list)\n",
    "#             print 'mer length', len(mer)\n",
    "            if len(mer_list) != len(mer):\n",
    "                print k, i\n",
    "                print ons[i] + '_count'\n",
    "                print '----------------------'\n",
    "#                 print mer\n",
    "            feature = ons[i] + '_count'\n",
    "            mer[feature] = mer_list\n",
    "\n",
    "\n",
    "        mer['total_count'] = mer['ft_1_count'] + mer['ft_2_count'] + mer['ft_3_count'] + \\\n",
    "                                    mer['ft_4_count'] + mer['ft_5_count'] + mer['ft_6_count']\n",
    "        mer['total_count'] = mer['total_count'] / 6\n",
    "        mer['is_hour_8'] = (mer['hour'] == 8)\n",
    "        mer['is_hour_9'] = (mer['hour'] == 9)\n",
    "        mer['is_hour_17'] = (mer['hour'] == 17)\n",
    "        mer['is_hour_18'] = (mer['hour'] == 18)\n",
    "        mer['is_min_0'] = (mer['rounded_min'] == 0)\n",
    "        mer['is_min_20'] = (mer['rounded_min'] == 20)\n",
    "        mer['is_min_40'] = (mer['rounded_min'] == 40)\n",
    "        mer['month_day'] = mer['month'].astype(str) + mer['day'].astype(str)\n",
    "\n",
    "        # Fill NAN with mean here\n",
    "        mer = mer.fillna(mer.mean())\n",
    "        \n",
    "\n",
    "        mers.append(mer)\n",
    "    return mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mers = create_prev_count(datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Add 'ft_x_ratio' features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_ft_ratio(datas, mers):\n",
    "    \n",
    "    mer_aves = []\n",
    "    \n",
    "    for i in range(5):\n",
    "        data = datas[i]\n",
    "        mer = mers[i]\n",
    "        \n",
    "        morning = data[data['hour'].astype(int).isin([6, 7])]\n",
    "        afternoon = data[data['hour'].astype(int).isin([15, 16])]\n",
    "\n",
    "#         hours_mor = [6, 7]\n",
    "#         hours_aft = [15, 16]\n",
    "#         minutes = [0, 20, 40]\n",
    "\n",
    "        mor_count = []\n",
    "        for hour in [6, 7]:\n",
    "            for minute in [0, 20, 40]:\n",
    "                part = morning[(morning['hour'] == hour) & (morning['rounded_min'] == minute)]\n",
    "                count = np.average(part['count'])\n",
    "                mor_count.append(count)\n",
    "\n",
    "        aft_count = []\n",
    "        for hour in [15, 16]:\n",
    "            for minute in [0, 20, 40]:\n",
    "                part = afternoon[(afternoon['hour'] == hour) & (afternoon['rounded_min'] == minute)]\n",
    "                count = np.average(part['count'])\n",
    "                aft_count.append(count)\n",
    "\n",
    "        total_count = np.average(afternoon['count'])\n",
    "        aft_count.append(total_count)\n",
    "\n",
    "        ave_matrix = []\n",
    "        for i in range(6):\n",
    "            ave_n_count = [mor_count[i], mor_count[i], aft_count[i], aft_count[i]]\n",
    "            ave_matrix.append(ave_n_count)\n",
    "\n",
    "        ave_count = [np.average(mor_count), np.average(mor_count), np.average(aft_count), np.average(aft_count)]\n",
    "        ave_matrix.append(ave_count)\n",
    "\n",
    "        ave_df = pd.DataFrame({'hour':[8, 9, 17, 18], 'ave_ft_1':ave_matrix[0], 'ave_ft_2':ave_matrix[1], \n",
    "                              'ave_ft_3':ave_matrix[2], 'ave_ft_4':ave_matrix[3], 'ave_ft_5':ave_matrix[4],\n",
    "                               'ave_ft_6':ave_matrix[5], 'ave_total_count':ave_matrix[6]})\n",
    "\n",
    "        mer_ave = pd.merge(left=mer, right=ave_df, on='hour', how='left', suffixes=('', '_r_'))\n",
    "\n",
    "        ratio_features = ['ft_1_ratio', 'ft_2_ratio', 'ft_3_ratio', 'ft_4_ratio', 'ft_5_ratio', 'ft_6_ratio', 'total_ratio']\n",
    "        ave_featurers = ['ave_ft_1', 'ave_ft_2', 'ave_ft_3', 'ave_ft_4', 'ave_ft_5', 'ave_ft_6', 'ave_total_count']\n",
    "        ft_features = ['ft_1_count', 'ft_2_count', 'ft_3_count', 'ft_4_count', 'ft_5_count', 'ft_6_count', 'total_count']\n",
    "\n",
    "        for i in range(7):\n",
    "            mer_ave[ratio_features[i]] = mer_ave[ft_features[i]] / mer_ave[ave_featurers[i]]\n",
    "            \n",
    "        mer_aves.append(mer_ave)\n",
    "        \n",
    "    return mer_aves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mers = create_ft_ratio(datas, mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_is_hour_min(mers):\n",
    "    for i in range(len(mers)):\n",
    "        df = mers[i]\n",
    "        df['hour'] = df['hour']\n",
    "        df['rounded_min'] =df['rounded_min']\n",
    "        hours = [8, 9, 17, 18]\n",
    "        minutes = [0, 20, 40]\n",
    "        for hour in hours:\n",
    "            for minute in minutes:\n",
    "                new_feature = 'is_hour_' + str(hour) + '_min_' + str(minute)\n",
    "                df[new_feature] = np.array([0] * len(df))\n",
    "                df.loc[(df['hour'].astype(int) == hour) & (df['rounded_min'].astype(int) == minute), new_feature] = 1\n",
    "        mers[i] = df\n",
    "    return mers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. find data that's in hour 7 and hour 8 \n",
    "2. check which makes more sense, average or percentile 50\n",
    "3. compute average of 'ft_1_count', for the moring time slot and afternoon time slot seperately\n",
    "    get 14 results in total. (7 features here, including 'total_count_ave'\n",
    "4. compute and add ratio as a new feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determine features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "features = ['ft_1_count', 'ft_2_count', 'ft_3_count', 'ft_4_count', 'ft_5_count', 'ft_6_count', 'total_count', \n",
    "           'is_hour_8', 'is_hour_9', 'is_hour_17', 'is_hour_18', 'is_min_0', 'is_min_20', 'is_min_40']\n",
    "\n",
    "# features.remove('total_count')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ratio_features = ['ft_1_ratio', 'ft_2_ratio', 'ft_3_ratio', 'ft_4_ratio', 'ft_5_ratio', 'ft_6_ratio', 'total_ratio']\n",
    "# features = features + ratio_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = features + weather_features\n",
    "# features.remove('window_time_formatted')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# features = ['ft_1_count', 'ft_2_count', 'ft_3_count', 'ft_4_count', 'ft_5_count', 'ft_6_count', 'total_count', \n",
    "#            'is_hour_8_min_0', 'is_hour_8_min_20', 'is_hour_8_min_40',\n",
    "#             'is_hour_9_min_0', 'is_hour_9_min_20', 'is_hour_9_min_40',\n",
    "#             'is_hour_17_min_0', 'is_hour_17_min_20', 'is_hour_17_min_40', \n",
    "#              'is_hour_18_min_0', 'is_hour_18_min_20', 'is_hour_18_min_40']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split and normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_sets = []\n",
    "hours = [8, 9, 17, 18]\n",
    "for mer in mers:\n",
    "    for hour in hours:\n",
    "        h_spec = mer[mer['hour'] == hour]\n",
    "        data_sets.append(h_spec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# from sklearn.preprocessing import normalize\n",
    "\n",
    "# to_norm_featurs = ['ft_1_count', 'ft_2_count', 'ft_3_count', 'ft_4_count', \n",
    "#                   'ft_5_count', 'ft_6_count', 'total_count']\n",
    "\n",
    "# for data in data_sets:\n",
    "#     for feature in to_norm_featurs:\n",
    "#         data[feature] = normalize(data[feature])[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split into training and test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def split_train_test(data_sets, features):\n",
    "    Xs_train_valid = []\n",
    "    Ys_train_valid = []\n",
    "    Xs_test = []\n",
    "    Ys_test = []\n",
    "    for i in range(20):\n",
    "        df = data_sets[i]\n",
    "\n",
    "        month_day = df['month_day'].unique()\n",
    "        random.shuffle(month_day)\n",
    "        test_size = int(0.25 * len(month_day))\n",
    "        test_index = month_day[:test_size]\n",
    "        train_index = month_day[test_size:]\n",
    "    #     print len(test_index)\n",
    "    #     print len(train_index)\n",
    "        train = df[df['month_day'].isin(train_index)]\n",
    "        test = df[df['month_day'].isin(test_index)]\n",
    "\n",
    "        Xs_train_valid.append(train[features])\n",
    "        Ys_train_valid.append(train['count'])\n",
    "        Xs_test.append(test[features])\n",
    "        Ys_test.append(test['count'])\n",
    "    return (Xs_train_valid, Ys_train_valid, Xs_test, Ys_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test) = split_train_test(data_sets, features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fit Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "\n",
    "def fit_model_LinearReg(X_train_valid, Y_train_valid):\n",
    "    \n",
    "    preds = []\n",
    "    mapes = []\n",
    "    regs = []\n",
    "    \n",
    "    seed = randint(0, 50)\n",
    "    X_train_valid = X_train_valid.sample(frac=1, random_state=seed)\n",
    "    Y_train_valid = Y_train_valid.sample(frac=1, random_state=seed)\n",
    "    \n",
    "    X_train_valid.index = range(len(X_train_valid))\n",
    "    Y_train_valid.index = range(len(Y_train_valid))\n",
    "\n",
    "    alphas = np.arange(0.1, 10, 0.1)\n",
    "    for a in alphas:\n",
    "        \n",
    "        \n",
    "        kf = KFold(n_splits=5)\n",
    "        mape_list = []\n",
    "        for t_index, v_index in kf.split(X_train_valid):\n",
    "            \n",
    "            train_X = X_train_valid.loc[t_index]\n",
    "            valid_X = X_train_valid.loc[v_index]\n",
    "            train_Y = Y_train_valid.loc[t_index]\n",
    "            valid_Y = Y_train_valid.loc[v_index]\n",
    "            \n",
    "            reg = linear_model.Ridge(alpha=a)\n",
    "            reg.fit(train_X, train_Y)\n",
    "            pred = reg.predict(valid_X)\n",
    "            mape = MAPE(pred, valid_Y)\n",
    "            mape_list.append(mape)\n",
    "        mapes.append(np.average(mape_list))\n",
    "        \n",
    "    index = np.array(mapes).argmin()\n",
    "    alpha = alphas[index]\n",
    "    mape = mapes[index]\n",
    "    reg = linear_model.Ridge(alpha=alpha)\n",
    "    reg.fit(X_train_valid, Y_train_valid)\n",
    "    \n",
    "    return (alpha, mape, reg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn import linear_model\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_LR_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=True):\n",
    "    total_alphas = []\n",
    "#     mapes_validation = []\n",
    "    LR_models = []\n",
    "    total_val_mapes = []\n",
    "    total_test_mapes = []\n",
    "    for i in range(20):\n",
    "        (alpha, mape_valid, reg) = fit_model_LinearReg(Xs_train_valid[i], Ys_train_valid[i])\n",
    "\n",
    "#         alphas.append(alpha)\n",
    "#         mapes_validation.append(mape_valid)\n",
    "        LR_models.append(reg)\n",
    "\n",
    "        X_test = Xs_test[i]\n",
    "        Y_test = Ys_test[i]\n",
    "        pred = reg.predict(X_test)\n",
    "        mape_test = MAPE(pred, Y_test)\n",
    "\n",
    "        total_val_mapes.append(mape_valid)\n",
    "        total_test_mapes.append(mape_test)\n",
    "        total_alphas.append(alpha)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('alpha: {0:.4f}'.format(alpha), 'mape_val: {0:.4f}'.format(mape_valid),\n",
    "                   'mape_test: {0:.4f}'.format(mape_test))\n",
    "    ave_mape_valid = np.average(total_val_mapes)\n",
    "    ave_mape_test = np.average(total_test_mapes)\n",
    "    ave_alpha = np.average(total_alphas)\n",
    "    print 'Validation mape:', '{0:.4f}'.format(ave_mape_valid)\n",
    "    print 'Test mape:', '{0:.4f}'.format(ave_mape_test)\n",
    "    print 'Alpha:', '{0:.4f}'.format(ave_alpha)\n",
    "    print '-------------------End of training---------------------------'\n",
    "    return (ave_mape_valid, ave_mape_test, LR_models, ave_alpha)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print linear regression model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without norm, without weather features\n",
    "# average validation mape is: 0.20\n",
    "# average test mape is: 0.22\n",
    "# average alpha is: 6.75\n",
    "\n",
    "# without norm, with weather features:\n",
    "# average validation mape is: 0.1782\n",
    "# average test mape is: 0.2627\n",
    "# average alpha is: 4.6640\n",
    "\n",
    "# without norm, without weather features, with 'total_count' / 6\n",
    "# average validation mape is: 0.1975\n",
    "# average test mape is: 0.2422\n",
    "# average alpha is: 7.1140\n",
    "\n",
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-39-e267b6ec2ce6>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m     \u001b[1;33m(\u001b[0m\u001b[0mXs_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs_test\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msplit_train_test\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_sets\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeatures\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m     \u001b[1;33m(\u001b[0m\u001b[0mmape_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmape_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLR_models\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[1;33m)\u001b[0m             \u001b[1;33m=\u001b[0m \u001b[0mtrain_LR_model_and_print\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs_train_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mXs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m     \u001b[0mmape_valid_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape_valid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mmape_test_list\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmape_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-37-e7f278762c33>\u001b[0m in \u001b[0;36mtrain_LR_model_and_print\u001b[1;34m(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose)\u001b[0m\n\u001b[0;32m      9\u001b[0m     \u001b[0mtotal_test_mapes\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m20\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m         \u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmape_valid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mreg\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_model_LinearReg\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mXs_train_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mYs_train_valid\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;31m#         alphas.append(alpha)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-36-8363b484ede4>\u001b[0m in \u001b[0;36mfit_model_LinearReg\u001b[1;34m(X_train_valid, Y_train_valid)\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[0mvalid_X\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX_train_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     26\u001b[0m             \u001b[0mtrain_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mt_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 27\u001b[1;33m             \u001b[0mvalid_Y\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mY_train_valid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mv_index\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     29\u001b[0m             \u001b[0mreg\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlinear_model\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mRidge\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0malpha\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, key)\u001b[0m\n\u001b[0;32m   1310\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_tuple\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1311\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1312\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1313\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1314\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_getitem_axis\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_getitem_axis\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1470\u001b[0m                     \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Cannot index with multidimensional key'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1472\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_getitem_iterable\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1473\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1474\u001b[0m             \u001b[1;31m# nested tuple slicing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\indexing.pyc\u001b[0m in \u001b[0;36m_getitem_iterable\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1071\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1072\u001b[0m                     result = self.obj.reindex_axis(keyarr, axis=axis,\n\u001b[1;32m-> 1073\u001b[1;33m                                                    level=level)\n\u001b[0m\u001b[0;32m   1074\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1075\u001b[0m                     \u001b[1;31m# this is an error as we are trying to find\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mreindex_axis\u001b[1;34m(self, labels, axis, **kwargs)\u001b[0m\n\u001b[0;32m   2378\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2379\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"cannot reindex series on non-zero axis!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2380\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlabels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2381\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2382\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mmemory_usage\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdeep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\series.pyc\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, index, **kwargs)\u001b[0m\n\u001b[0;32m   2360\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'reindex'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2361\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2362\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mSeries\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreindex\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2364\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mAppender\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgeneric\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_shared_docs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'fillna'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0m_shared_doc_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36mreindex\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   2257\u001b[0m         \u001b[1;31m# perform the reindex on the axes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2258\u001b[0m         return self._reindex_axes(axes, level, limit, tolerance, method,\n\u001b[1;32m-> 2259\u001b[1;33m                                   fill_value, copy).__finalize__(self)\n\u001b[0m\u001b[0;32m   2260\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2261\u001b[0m     def _reindex_axes(self, axes, level, limit, tolerance, method, fill_value,\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_reindex_axes\u001b[1;34m(self, axes, level, limit, tolerance, method, fill_value, copy)\u001b[0m\n\u001b[0;32m   2275\u001b[0m             obj = obj._reindex_with_indexers({axis: [new_index, indexer]},\n\u001b[0;32m   2276\u001b[0m                                              \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2277\u001b[1;33m                                              copy=copy, allow_dups=False)\n\u001b[0m\u001b[0;32m   2278\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2279\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\THINKPAD\\Anaconda2\\lib\\site-packages\\pandas\\core\\generic.pyc\u001b[0m in \u001b[0;36m_reindex_with_indexers\u001b[1;34m(self, reindexers, fill_value, copy, allow_dups)\u001b[0m\n\u001b[0;32m   2369\u001b[0m                                                 \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfill_value\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2370\u001b[0m                                                 \u001b[0mallow_dups\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_dups\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2371\u001b[1;33m                                                 copy=copy)\n\u001b[0m\u001b[0;32m   2372\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2373\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcopy\u001b[0m \u001b[1;32mand\u001b[0m \u001b[0mnew_data\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "mape_valid_list = []\n",
    "mape_test_list = []\n",
    "alpha_list = []\n",
    "LR_model_list = []\n",
    "for i in range(5):\n",
    "    (Xs_train_valid, Ys_train_valid, Xs_test, Ys_test) = split_train_test(data_sets, features)\n",
    "    (mape_valid, mape_test, LR_models, alpha) \\\n",
    "            = train_LR_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=False)\n",
    "    mape_valid_list.append(mape_valid)\n",
    "    mape_test_list.append(mape_test)\n",
    "    alpha_list.append(alpha)\n",
    "    LR_model_list.append(LR_models)\n",
    "print 'average validation mape is:', '{0:.4f}'.format(np.average(mape_valid_list))\n",
    "print 'average test mape is:', '{0:.4f}'.format(np.average(mape_test_list))\n",
    "print 'average alpha is:', '{0:.4f}'.format(np.average(alpha_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def fit_model_DT(X_train_valid, Y_train_valid):\n",
    "    \n",
    "    mapes = []\n",
    "    regs = []\n",
    "    \n",
    "    seed = randint(0, 50)\n",
    "    X_train_valid = X_train_valid.sample(frac=1, random_state=seed)\n",
    "    Y_train_valid = Y_train_valid.sample(frac=1, random_state=seed)\n",
    "    \n",
    "    X_train_valid.index = range(len(X_train_valid))\n",
    "    Y_train_valid.index = range(len(Y_train_valid))\n",
    "    \n",
    "    kf = KFold(n_splits=10)\n",
    "    \n",
    "    \n",
    "    for i in range(15):\n",
    "        \n",
    "        depth = 1+i\n",
    "        mape_list = []\n",
    "        \n",
    "        for t_index, v_index in kf.split(X_train_valid):\n",
    "\n",
    "            train_X = X_train_valid.loc[t_index]\n",
    "            valid_X = X_train_valid.loc[v_index]\n",
    "            train_Y = Y_train_valid.loc[t_index]\n",
    "            valid_Y = Y_train_valid.loc[v_index]\n",
    "\n",
    "            reg = DecisionTreeRegressor(max_depth=depth)\n",
    "            reg.fit(train_X, train_Y)\n",
    "            pred = reg.predict(valid_X)\n",
    "            mape_list.append(MAPE(pred, valid_Y))\n",
    "            \n",
    "        mapes.append(np.average(mape_list))\n",
    "    \n",
    "    index = np.array(mapes).argmin()\n",
    "    depth = index + 1\n",
    "    mape = np.average(mape_list)\n",
    "    reg = DecisionTreeRegressor(max_depth=depth)\n",
    "    reg.fit(X_train_valid,Y_train_valid)\n",
    "    \n",
    "    return (depth, mape, reg)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_DT_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=True):\n",
    "    total_depths = []\n",
    "    DT_models = []\n",
    "    total_val_mapes = []\n",
    "    total_test_mapes = []\n",
    "    for i in range(20):\n",
    "        (depth, mape_valid, reg) = fit_model_DT(Xs_train_valid[i], Ys_train_valid[i])\n",
    "\n",
    "        DT_models.append(reg)\n",
    "\n",
    "        X_test = Xs_test[i]\n",
    "        Y_test = Ys_test[i]\n",
    "        pred = reg.predict(X_test)\n",
    "        mape_test = MAPE(pred, Y_test)\n",
    "\n",
    "        total_val_mapes.append(mape_valid)\n",
    "        total_test_mapes.append(mape_test)\n",
    "        total_depths.append(depth)\n",
    "        \n",
    "        if verbose:\n",
    "            print ('depth: {0:.4f}'.format(depth), 'mape_val: {0:.4f}'.format(mape_valid),\n",
    "                   'mape_test: {0:.4f}'.format(mape_test))\n",
    "    ave_mape_valid = np.average(total_val_mapes)\n",
    "    ave_mape_test = np.average(total_test_mapes)\n",
    "    ave_depth = np.average(total_depths)\n",
    "    print 'Validation mape:', '{0:.4f}'.format(ave_mape_valid)\n",
    "    print 'Test mape:', '{0:.4f}'.format(ave_mape_test)\n",
    "    print 'Depth:', '{0:.4f}'.format(ave_depth)\n",
    "    print '-------------------End of training---------------------------'\n",
    "    return (ave_mape_valid, ave_mape_test, DT_models, ave_depth)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Decision Tree Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without norm, without weather features\n",
    "# average validation mape is: 0.22\n",
    "# average test mape is: 0.22\n",
    "# average depth is: 3.19\n",
    "\n",
    "# without norm, with weather features\n",
    "# average validation mape is: 0.2270\n",
    "# average test mape is: 0.2292\n",
    "# average depth is: 3.2850\n",
    "\n",
    "# without norm, without weather features, with 'total_count' / 6\n",
    "# average validation mape is: 0.2322\n",
    "# average test mape is: 0.2035\n",
    "# average depth is: 2.8800"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mape_valid_list = []\n",
    "mape_test_list = []\n",
    "depth_list = []\n",
    "DT_model_list = []\n",
    "for i in range(5):\n",
    "    (Xs_train_valid, Ys_train_valid, Xs_test, Ys_test) = split_train_test(data_sets, features)\n",
    "    (mape_valid, mape_test, DT_models, depth) \\\n",
    "            = train_DT_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=False)\n",
    "    mape_valid_list.append(mape_valid)\n",
    "    mape_test_list.append(mape_test)\n",
    "    depth_list.append(depth)\n",
    "    DT_model_list.append(DT_models)\n",
    "print 'average validation mape is:', '{0:.4f}'.format(np.average(mape_valid_list))\n",
    "print 'average test mape is:', '{0:.4f}'.format(np.average(mape_test_list))\n",
    "print 'average depth is:', '{0:.4f}'.format(np.average(depth_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from random import randint\n",
    "\n",
    "\n",
    "def fit_model_RF(X_train_valid, Y_train_valid):\n",
    "    \n",
    "    initial_depth = 1\n",
    "    depth = initial_depth\n",
    "    initial_est = 5\n",
    "    est = initial_est\n",
    "    \n",
    "    mapes = []\n",
    "    regs = []\n",
    "    \n",
    "    seed = randint(0, 50)\n",
    "    X_train_valid = X_train_valid.sample(frac=1, random_state=seed)\n",
    "    Y_train_valid = Y_train_valid.sample(frac=1, random_state=seed)\n",
    "    \n",
    "    X_train_valid.index = range(len(X_train_valid))\n",
    "    Y_train_valid.index = range(len(Y_train_valid))\n",
    "    \n",
    "    kf = KFold(n_splits=5)\n",
    "    \n",
    "    \n",
    "    for i in range(10):\n",
    "        \n",
    "#         depth = depth + i\n",
    "        mape_list = []\n",
    "        est = est + 5 * i\n",
    "        \n",
    "        for t_index, v_index in kf.split(X_train_valid):\n",
    "\n",
    "            train_X = X_train_valid.loc[t_index]\n",
    "            valid_X = X_train_valid.loc[v_index]\n",
    "            train_Y = Y_train_valid.loc[t_index]\n",
    "            valid_Y = Y_train_valid.loc[v_index]\n",
    "\n",
    "#             reg = RandomForestRegressor(max_depth=depth)\n",
    "            reg = RandomForestRegressor(n_estimators=est)\n",
    "            reg.fit(train_X, train_Y)\n",
    "            pred = reg.predict(valid_X)\n",
    "            mape_list.append(MAPE(pred, valid_Y))\n",
    "            \n",
    "        mapes.append(np.average(mape_list))\n",
    "    \n",
    "    index = np.array(mapes).argmin()\n",
    "#     depth = index + 1\n",
    "    est = initial_est + 5 * index\n",
    "    mape = np.average(mape_list)\n",
    "    reg = RandomForestRegressor(n_estimators=est)\n",
    "    reg.fit(X_train_valid,Y_train_valid)\n",
    "    \n",
    "    return (est, mape, reg)               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import KFold\n",
    "\n",
    "def train_RF_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=True):\n",
    "#     total_depths = []\n",
    "    total_ests = []\n",
    "    RF_models = []\n",
    "    total_val_mapes = []\n",
    "    total_test_mapes = []\n",
    "    for i in range(20):\n",
    "        (est, mape_valid, reg) = fit_model_RF(Xs_train_valid[i], Ys_train_valid[i])\n",
    "\n",
    "        RF_models.append(reg)\n",
    "\n",
    "        X_test = Xs_test[i]\n",
    "        Y_test = Ys_test[i]\n",
    "        pred = reg.predict(X_test)\n",
    "        mape_test = MAPE(pred, Y_test)\n",
    "\n",
    "        total_val_mapes.append(mape_valid)\n",
    "        total_test_mapes.append(mape_test)\n",
    "#         total_depths.append(depth)\n",
    "        total_ests.append(est)\n",
    "        \n",
    "        if verbose:\n",
    "#             print ('depth: {0:.4f}'.format(depth), 'mape_val: {0:.4f}'.format(mape_valid),\n",
    "#                    'mape_test: {0:.4f}'.format(mape_test))\n",
    "            print ('n_estimator: {0:.4f}'.format(est), 'mape_val: {0:.4f}'.format(mape_valid),\n",
    "                   'mape_test: {0:.4f}'.format(mape_test))\n",
    "    ave_mape_valid = np.average(total_val_mapes)\n",
    "    ave_mape_test = np.average(total_test_mapes)\n",
    "#     ave_depth = np.average(total_depths)\n",
    "    ave_est = np.average(total_ests)\n",
    "    print 'Validation mape:', '{0:.4f}'.format(ave_mape_valid)\n",
    "    print 'Test mape:', '{0:.4f}'.format(ave_mape_test)\n",
    "#     print 'Depth:', '{0:.4f}'.format(ave_depth)\n",
    "    print 'n_estimators:', '{0:.4f}'.format(ave_est)\n",
    "    print '-------------------End of training---------------------------'\n",
    "    return (ave_mape_valid, ave_mape_test, RF_models, ave_est)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print Random Forest model results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# without norm, without weather features\n",
    "# average validation mape is: 0.19\n",
    "# average test mape is: 0.19\n",
    "# average depth is: 7.82\n",
    "\n",
    "# without norm, with weather features\n",
    "# average validation mape is: 0.1864\n",
    "# average test mape is: 0.2212\n",
    "# average depth is: 7.9000\n",
    "\n",
    "# without norm, without weather features, with 'total_count' / 6\n",
    "# average validation mape is: 0.1808\n",
    "# average test mape is: 0.2045\n",
    "# average depth is: 7.4400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "mape_valid_list = []\n",
    "mape_test_list = []\n",
    "# depth_list = []\n",
    "n_estimator_list = []\n",
    "RF_model_list = []\n",
    "for i in range(1):\n",
    "    (Xs_train_valid, Ys_train_valid, Xs_test, Ys_test) = split_train_test(data_sets, features)\n",
    "    (mape_valid, mape_test, RF_models, est) \\\n",
    "            = train_RF_model_and_print(Xs_train_valid, Ys_train_valid, Xs_test, Ys_test, verbose=True)\n",
    "    mape_valid_list.append(mape_valid)\n",
    "    mape_test_list.append(mape_test)\n",
    "#     depth_list.append(depth)\n",
    "    n_estimator_list.append(est)\n",
    "    RF_model_list.append(RF_models)\n",
    "print 'average validation mape is:', '{0:.4f}'.format(np.average(mape_valid_list))\n",
    "print 'average test mape is:', '{0:.4f}'.format(np.average(mape_test_list))\n",
    "# print 'average depth is:', '{0:.4f}'.format(np.average(depth_list))\n",
    "print 'average n_estimator is:', '{0:.4f}'.format(np.average(n_estimator_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breakbopint here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "forest = RF_model_list[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "importances = forest.feature_importances_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "std = np.std([tree.feature_importances_ for tree in forest.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X = Xs_train_valid[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "create_RF_importance_df(forest, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for f in range(X.shape[1]):\n",
    "    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.bar(range(X.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X.shape[1]), indices)\n",
    "plt.xlim([-1, X.shape[1]])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "breakbopint here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_RF_importance_df(forest, features):\n",
    "    importance = forest.feature_importances_\n",
    "    df = pd.DataFrame(data={'importance':importance, 'feature':features})\n",
    "    df.sort('importance', ascending=False, inplace=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_coef_df(reg_model, features):\n",
    "    coef_df = pd.DataFrame(data={'coef':reg_model.coef_, 'feature':features})\n",
    "    coef_df.sort('coef', ascending=False, inplace=True)\n",
    "    return coef_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make submission"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_combi_in = pd.read_csv(\"sub_combi_in.csv\")\n",
    "sub_combi_out = pd.read_csv(\"sub_combi_out.csv\")\n",
    "test_combi_in = pd.read_csv('test_combi_in.csv')\n",
    "test_combi_out = pd.read_csv('test_combi_out.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_combi_in = pd.concat([sub_combi_in, test_combi_in])\n",
    "sub_combi_out = pd.concat([sub_combi_out, test_combi_out])\n",
    "\n",
    "useful_features = ['window_time', 'window_time_formatted', 'month', 'day', 'hour', 'tollgate_id', 'direction', \n",
    "                            'count', 'minute']\n",
    "sub_combi_in = sub_combi_in[useful_features]\n",
    "sub_combi_out = sub_combi_out[useful_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_combi_in['rounded_min'] = (sub_combi_in['minute'].astype(int) // 20 * 20).astype(str)\n",
    "sub_combi_out['rounded_min'] = (sub_combi_out['minute'].astype(int) // 20 * 20).astype(str)\n",
    "\n",
    "sub_combi_in['window_time_formatted'] = sub_combi_in['window_time_formatted'].apply(\n",
    "                    lambda t : datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')).tolist()\n",
    "sub_combi_out['window_time_formatted'] = sub_combi_out['window_time_formatted'].apply(\n",
    "                    lambda t : datetime.datetime.strptime(t, '%Y-%m-%d %H:%M:%S')).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# # Merge data\n",
    "# sub_combi_in = pd.merge(sub_combi_in, sub_gp_traj[['window_time_formatted', 'tollgate_id', 'direction']],\n",
    "#              on=['window_time_formatted', 'tollgate_id', 'direction'], suffixes=('', '_y'), how='left')\n",
    "\n",
    "\n",
    "# Split data by 'tollgate_id'\n",
    "sub_combi_in_1 = sub_combi_in[sub_combi_in['tollgate_id'] == 1]\n",
    "sub_combi_in_2 = sub_combi_in[sub_combi_in['tollgate_id'] == 2]\n",
    "sub_combi_in_3 = sub_combi_in[sub_combi_in['tollgate_id'] == 3]\n",
    "sub_combi_out_1 = sub_combi_out[sub_combi_out['tollgate_id'] == 1]\n",
    "sub_combi_out_3 = sub_combi_out[sub_combi_out['tollgate_id'] == 3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Process data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_datas = [sub_combi_in_1, sub_combi_in_2, sub_combi_in_3, sub_combi_out_1, sub_combi_out_3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_datas = create_shifted_time(sub_datas)\n",
    "sub_mers = create_prev_count(sub_datas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_data_sets = []\n",
    "hours = [8, 9, 17, 18]\n",
    "for mer in sub_mers:\n",
    "    for hour in hours:\n",
    "        h_spec = mer[mer['hour'] == hour]\n",
    "        sub_data_sets.append(h_spec)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for i in range(20):\n",
    "    model = LR_model_list[0][i]\n",
    "    pred = model.predict(sub_data_sets[i][features])\n",
    "    sub_data_sets[i]['preds'] = pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict using history data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_pred = pd.concat(sub_data_sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi = pd.concat(mers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sub_pred['preds'] = np.array([0] * len(sub_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_pred.index = np.array(range(len(sub_pred)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "combi = combi[['day', 'hour', 'rounded_min', 'count', 'tollgate_id', 'direction']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi = combi.groupby(by=['hour', 'rounded_min', 'tollgate_id', 'direction']).agg('mean')\n",
    "combi['hour'] = combi.index.get_level_values('hour')\n",
    "combi['rounded_min'] = combi.index.get_level_values('rounded_min')\n",
    "combi['tollgate_id'] = combi.index.get_level_values('tollgate_id')\n",
    "combi['direction'] = combi.index.get_level_values('direction')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "combi['minute'] = combi['rounded_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_pred = pd.merge(sub_pred, combi, how='left', on=['hour', 'minute', 'tollgate_id', 'direction'], suffixes=('', '_y'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sub_pred['preds'] = sub_pred['count_y']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process sample submission dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.read_csv('submission_sample_volume.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def submission_time_converter(df):\n",
    "    df['time_formatted'] = df['time_window'].apply(\n",
    "    lambda t : datetime.datetime.strptime(t.split(',')[0], '[%Y-%m-%d %H:%M:%S'))\n",
    "    df['year'] = df['time_formatted'].apply(lambda t : t.year).astype(int)\n",
    "    df['month'] = df['time_formatted'].apply(lambda t : t.month).astype(int)\n",
    "    df['day'] = df['time_formatted'].apply(lambda t : t.day).astype(int)\n",
    "    df['hour'] = df['time_formatted'].apply(lambda t : t.hour).astype(int)\n",
    "    df['minute'] = df['time_formatted'].apply(lambda t : t.minute).astype(int)\n",
    "    df['rounded_min'] = (df['minute'].astype(int) // 20 * 20).astype(int)\n",
    "    slash = np.array(['-'] * len(df))\n",
    "    df['window_time'] = df['year'].astype(str) + slash + df['month'].astype(str) + slash + df['day'].astype(str) \\\n",
    "                            + slash + df['hour'].astype(str) + slash + df['rounded_min'].astype(str)\n",
    "    df['window_time_formatted'] = df['window_time'].apply(lambda t : datetime.datetime.strptime(t, '%Y-%m-%d-%H-%M'))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "submission = submission_time_converter(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Merge sample data and prediction\n",
    "my_submission = pd.merge(submission, sub_pred[['tollgate_id', 'direction', 'preds', 'window_time']],\n",
    "                         on=['tollgate_id', 'direction', 'window_time'], how='left')\n",
    "\n",
    "# Delete not useful data\n",
    "my_submission = my_submission[['tollgate_id', 'direction', 'time_window', 'preds']]\n",
    "\n",
    "# Rename column name\n",
    "my_submission.rename(columns={'preds':'volume'}, inplace=True)\n",
    "\n",
    "# Convert data type\n",
    "my_submission['volume'] = my_submission['volume'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adjust volumn sequence\n",
    "my_submission = my_submission[['tollgate_id', 'time_window', 'direction', 'volume']]\n",
    "\n",
    "# Save data\n",
    "my_submission.to_csv(\"predictions/history_prediction_503.csv\", index=False)\n",
    "\n",
    "# Load to check\n",
    "my_submission = pd.read_csv(\"predictions/history_prediction_503.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ori = pd.read_csv('predictions/LR_prediction_423.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.corrcoef(ori['volume'], my_submission['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAPE(ori['volume'], my_submission['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(16, 10))\n",
    "plt.plot(range(420), ori['volume'], '.', \n",
    "        range(420), my_submission['volume'], '*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "RF_pred = pd.read_csv(\"predictions/RF_prediction_427_2.csv\")\n",
    "DT_pred = pd.read_csv(\"predictions/DT_prediction_427.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.corrcoef(RF_pred['volume'], DT_pred['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "MAPE(RF_pred['volume'], DT_pred['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.average((RF_pred['volume'] - DT_pred['volume']) / DT_pred['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "np.average((DT_pred['volume'] - RF_pred['volume']) / RF_pred['volume'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(range(420), RF_pred['volume'], '.',\n",
    "        range(420), DT_pred['volume'], '*')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Observation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = datas[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "h6 = df[df['hour'].astype(int) == 6]\n",
    "h7 = df[df['hour'].astype(int) == 7]\n",
    "h8 = df[df['hour'].astype(int) == 8]\n",
    "h9 = df[df['hour'].astype(int) == 9]\n",
    "plt.figure(figsize=(12, 6))\n",
    "print len(h7)==len(h8)\n",
    "ratio = np.array(h8['count'].tolist()) / np.array(h7['count'].tolist())\n",
    "plt.plot(range(len(h7)), ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_ratio_trend(df, hour1, hour2):\n",
    "    h1_df = df[df['hour'].astype(int) == hour1][['day', 'count']]\n",
    "    h2_df = df[df['hour'].astype(int) == hour2][['day', 'count']]\n",
    "    length = len(h1_df)\n",
    "    ratio = np.array(h2_df['count'].tolist()) // np.array(h1_df['count'].tolist())\n",
    "    \n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.plot(length, ratio)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = datas[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.loc[df['hour'].astype(int).isin([7, 8])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plot_ratio_trend(datas[1], 7, 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
